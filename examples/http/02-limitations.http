# ============================================
# 大模型局限性演示（精简版）
# ============================================
# 说明：8 个请求，覆盖 5 大局限性 + 1 个解决方案
# 模型：glm-4-flash（无内置推理，能清晰暴露 LLM 原始局限性）
# 测试日期：2026-02-15
# ============================================

@LOCALE = zh-CN
@response_language = You must respond in the locale specified by the LOCALE variable ({{LOCALE}}).All your responses should be in the language corresponding to this locale code.Do not use any other language regardless of the user's input language.

@GLM_BASE_URL = https://open.bigmodel.cn/api/coding/paas/v4/chat/completions
@GLM_BEARER_TOKEN = Bearer {{$dotenv GLM_API_KEY}}

@DEEP_SEEK_BASE_URL = https://api.deepseek.com/v1/chat/completions
@DEEP_SEEK_BEARER_TOKEN = Bearer {{$dotenv DEEPSEEK_API_KEY}}
# StepFun
@STEPFUN_BASE_URL = https://api.stepfun.com/v1/chat/completions
@STEPFUN_BEARER_TOKEN = Bearer {{$dotenv STEPFUN_API_KEY}}

@intelligent_assistatnt_prompt = Your are intelligent assistant named Alice. Your main role is to help users answer their questions.

# ============================================
# 1. 数学计算 —— LLM 不是计算器
# ============================================

### [1.1] 复杂小数计算 - 数量级都能算错
# 正确答案（Python）：209,528,294,431.6329（约 2095 亿）
# glm-4-flash 回答：2,084,909,917（约 20 亿，差了约 100 倍）
POST {{GLM_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: {{GLM_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "system",
      "content": "{{response_language}}"
    },
    {
      "role": "user",
      "content": "请计算：982717.1211 * 213213.23321 - 321312.777 / 1112.1121. 不要使用任何工具，不要显示推理，直接给出最终数字。"
    }
  ],
  "max_tokens": 200,
  "model": "glm-4-flash",
  "stream": false,
  "temperature": 0
}

### [1.2] 解决方案：Function Calling + 计算器
# 同一个 glm-4-flash，给了工具就会调用，拿到精确结果
POST {{GLM_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: {{GLM_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "system",
      "content": "{{intelligent_assistatnt_prompt}} {{response_language}}"
    },
    {
      "role": "user",
      "content": "请计算：982717.1211 * 213213.23321 - 321312.777 / 1112.1121"
    }
  ],
  "model": "glm-4-flash",
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "calculator",
        "description": "执行数学计算表达式，返回精确结果",
        "parameters": {
          "type": "object",
          "properties": {
            "expression": {
              "type": "string",
              "description": "要计算的数学表达式"
            }
          },
          "required": ["expression"]
        }
      }
    }
  ],
  "stream": false,
  "temperature": 0
}

# ============================================
# 2. 知识截止 —— 训练数据有保质期
# ============================================

### [2.1] 知识截止 - 2025 年后事件
# glm-4-flash 回答：承认数据截止于 2023 年（表现正确）
POST {{GLM_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: {{GLM_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "system",
      "content": "你是一个知识助手。如果不知道答案，请明确告知。"
    },
    {
      "role": "user",
      "content": "请问 2026 年 2 月发生了哪些重大科技新闻？"
    }
  ],
  "model": "glm-4-flash",
  "stream": false,
  "temperature": 0
}

# ============================================
# 3. 逻辑推理 —— 形式逻辑是短板
# ============================================

### [3.1] 传递性排序 - 完全搞反
# 正确答案：C 比 A 高（A<B<C<D<E）
# glm-4-flash 回答：排成 "D>E>A>B>C"，结论 "C 比 A 矮"（完全反了）
POST {{GLM_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: {{GLM_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "system",
      "content": "你是一个逻辑推理助手。"
    },
    {
      "role": "user",
      "content": "给定：\n1. A 比 B 矮\n2. B 比 C 矮\n3. C 比 D 矮\n4. D 比 E 矮\n\n请问：C 和 A 谁更高？"
    }
  ],
  "model": "glm-4-flash",
  "stream": false,
  "temperature": 0
}

# ============================================
# 4. 幻觉 —— 最顽固的问题
# ============================================

### [4.1] 事实混淆 - 英文绕过幻觉防护
# 中文问"林黛玉倒拔垂杨柳"已免疫，换英文就失效
# 英文问"Who pulled down the willow tree in Dream of the Red Chamber？"
# glm-4-flash 回答：编造了"凤姐（王熙凤）命人砍掉了柳树"
POST {{GLM_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: {{GLM_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "system",
      "content": "{{response_language}}"
    },
    {
      "role": "user",
      "content": "在红楼梦中，谁倒拔了垂杨柳？直接回答我，不要解释"
    }
  ],
  "model": "glm-4-flash",
  "stream": false,
  "temperature": 0
}

### [4.2] 技术幻觉 - 编造新 API 用法
# Python 3.14 新增 compression.zstd 模块，模型训练数据里没有
# glm-4-flash 回答：用 import zstd（第三方包）冒充标准库，代码全是编的
POST {{GLM_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: {{GLM_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "system",
      "content": "{{response_language}} 直接回答，不用推理"
    },
    {
      "role": "user",
      "content": "Python 3.14 introduced a new compression.zstd module. Please explain how to use compression.zstd.compress() and show an example."
    }
  ],
  "model": "glm-4-flash",
  "stream": false,
  "temperature": 0
}

### [4.3] 编造统计数据
# 真实数据：ChatGPT 83.3% | GitHub Copilot 27.4% | Bing AI 19.5%
# glm-4-flash 回答：Copilot 35%、Kite 23%（2022 年已停服）、Replika 20%（聊天机器人）
# 验证：https://survey.stackoverflow.co/2023/
POST {{DEEP_SEEK_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: {{DEEP_SEEK_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "system",
      "content": "你是一个技术分析师，直接给出数据"
    },
    {
      "role": "user",
      "content": "请列出 2023 年 Stack Overflow 开发者调查中，使用率最高的 5 个 AI 编程工具及其具体使用率百分比。"
    }
  ],
  "model": "deepseek-chat",
  "stream": false,
  "temperature": 0
}

# ============================================
# 5. 计数 —— 靠感觉猜位置
# ============================================

### [5.1] 列表计数 - 数到第 15 个差 2 位
# 正确答案：济南（第 15 个）
# glm-4-flash 回答：长沙（第 13 个）
POST {{GLM_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: {{GLM_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "system",
      "content": "你是一个助手，请根据给定信息回答问题。"
    },
    {
      "role": "user",
      "content": "以下是一个城市列表：\n北京、上海、广州、深圳、杭州、南京、武汉、成都、重庆、西安、天津、苏州、长沙、郑州、济南、青岛、大连、沈阳、哈尔滨、长春、厦门、福州、昆明、贵阳、南宁、海口、兰州、银川、西宁、乌鲁木齐。\n\n请问：第 15 个城市是哪个？"
    }
  ],
  "model": "glm-4-flash",
  "stream": false,
  "temperature": 0
}

### 最近比较火的陷阱问题
POST {{GLM_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: {{GLM_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "system",
      "content": "你是一个助手，请根据给定信息回答问题。"
    },
    {
      "role": "user",
      "content": "我想去洗车，洗车店距离我家50米，你说我应该开车过去还是走过去？"
    }
  ],
  "model": "glm-5",
  "stream": false,
  "temperature": 0
}
# ============================================
# 总结
# ============================================
# 局限性        | 表现                  | 解决方案
# 数学计算      | 复杂运算差 100 倍      | Function Calling + 计算器
# 知识截止      | 训练后的事不知道        | MCP / Web Search
# 逻辑推理      | 传递排序完全搞反        | 推理模型 / CoT
# 幻觉          | 编统计、编 API、换语言绕过 | 验证 + 引用 + LSP
# 计数          | 30 个数第 15 个差 2 位  | 代码执行
#
# 核心认知：LLM 是语言专家，不是知识库，不是计算器。
# Function Calling 是弥补局限性的核心技术。
# ============================================
