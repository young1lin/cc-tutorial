# ============================================
# 参数实验与对比
# ============================================
# 说明：深入探索 LLM API 参数对输出的影响
# 目的：理解如何通过调整参数优化模型表现
# 模型：GLM-4.7
# 核心参数：temperature、top_p、max_tokens、frequency_penalty
# ============================================

# 环境变量
@GLM_BASE_URL = https://open.bigmodel.cn/api/coding/paas/v4/chat/completions
@GLM_BEARER_TOKEN = Bearer {{$dotenv GLM_API_KEY}}

# ============================================
# 1. Temperature 参数实验
# ============================================

### [参数 1.1] Temperature = 0（完全确定性）
# 总是选择概率最高的 token，每次运行结果相同
# 适用：测试、事实问答、代码生成
POST {{GLM_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: {{GLM_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "system",
      "content": "你是一个创意写作助手。"
    },
    {
      "role": "user",
      "content": "写一句赞美春天的诗。"
    }
  ],
  "model": "GLM-4.7",
  "stream": false,
  "temperature": 0
}

### [参数 1.2] Temperature = 0.7（平衡）
# 创造性和准确性的平衡点，大多数场景的默认选择
POST {{GLM_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: {{GLM_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "system",
      "content": "你是一个创意写作助手。"
    },
    {
      "role": "user",
      "content": "写一句赞美春天的诗。"
    }
  ],
  "model": "GLM-4.7",
  "stream": false,
  "temperature": 0.7
}

### [参数 1.3] Temperature = 1.5（极高创造性）
# 极高随机性，输出可能不连贯，适合实验性创意探索
POST {{GLM_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: {{GLM_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "system",
      "content": "你是一个创意写作助手。"
    },
    {
      "role": "user",
      "content": "写一句赞美春天的诗。"
    }
  ],
  "model": "GLM-4.7",
  "stream": false,
  "temperature": 1.5
}

# ============================================
# 2. Top-P (Nucleus Sampling) 参数实验
# ============================================

### [参数 2.1] Top-P = 0.1（极保守）
# 只考虑累积概率前 10% 的 token，大幅缩小候选词范围
POST {{GLM_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: {{GLM_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "user",
      "content": "描述一只猫。"
    }
  ],
  "model": "GLM-4.7",
  "stream": false,
  "temperature": 0.7,
  "top_p": 0.1
}

### [参数 2.2] Top-P = 0.9（平衡，默认值）
# 大多数场景的默认值，平衡多样性和连贯性
# 注意：通常不与 temperature 同时调整
POST {{GLM_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: {{GLM_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "user",
      "content": "描述一只猫。"
    }
  ],
  "model": "GLM-4.7",
  "stream": false,
  "temperature": 0.7,
  "top_p": 0.9
}

# ============================================
# 3. Max Tokens 参数实验
# ============================================

### [参数 3.1] Max Tokens = 50（极简短）
# 1 个中文字 ≈ 2-3 tokens，输出可能被截断
POST {{GLM_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: {{GLM_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "user",
      "content": "介绍一下人工智能。"
    }
  ],
  "model": "GLM-4.7",
  "stream": false,
  "temperature": 0,
  "max_tokens": 50
}

### [参数 3.2] Max Tokens = 1000（详细回答）
# 允许充分阐述，约 500-700 个中文字
POST {{GLM_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: {{GLM_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "user",
      "content": "介绍一下人工智能。"
    }
  ],
  "model": "GLM-4.7",
  "stream": false,
  "temperature": 0,
  "max_tokens": 1000
}

# ============================================
# 4. Frequency Penalty（频率惩罚）
# ============================================

### [参数 4.1] Frequency Penalty = 0（无惩罚）
# 不惩罚重复词汇，可能出现较多重复表达
POST {{GLM_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: {{GLM_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "user",
      "content": "写一段关于阳光的描述（100 字），尽可能生动。"
    }
  ],
  "model": "GLM-4.7",
  "stream": false,
  "temperature": 0.7,
  "frequency_penalty": 0
}

### [参数 4.2] Frequency Penalty = 1.0（强惩罚）
# 强力惩罚重复，词汇高度多样，但可能影响连贯性
POST {{GLM_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: {{GLM_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "user",
      "content": "写一段关于阳光的描述（100 字），尽可能生动。"
    }
  ],
  "model": "GLM-4.7",
  "stream": false,
  "temperature": 0.7,
  "frequency_penalty": 1.0
}

# ============================================
# 5. Presence Penalty（存在惩罚）
# ============================================

### [参数 5.1] Presence Penalty = 0（无惩罚）
# 区别于 frequency_penalty：presence 只看是否出现过，不看出现次数
POST {{GLM_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: {{GLM_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "user",
      "content": "写一篇关于咖啡的短文（100 字）。"
    }
  ],
  "model": "GLM-4.7",
  "stream": false,
  "temperature": 0.7,
  "presence_penalty": 0
}

### [参数 5.2] Presence Penalty = 1.0（中等惩罚）
# 鼓励引入新话题，内容更丰富
POST {{GLM_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: {{GLM_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "user",
      "content": "写一篇关于咖啡的短文（100 字）。"
    }
  ],
  "model": "GLM-4.7",
  "stream": false,
  "temperature": 0.7,
  "presence_penalty": 1.0
}

# ============================================
# 6. 多参数组合实验
# ============================================

### [组合 6.1] 创意写作最佳配置
# temperature=0.8, frequency_penalty=0.5 → 既有创意又避免重复
POST {{GLM_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: {{GLM_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "user",
      "content": "写一个科幻小说的开头（150 字）。"
    }
  ],
  "model": "GLM-4.7",
  "stream": false,
  "temperature": 0.8,
  "frequency_penalty": 0.5,
  "max_tokens": 300
}

### [组合 6.2] 技术文档最佳配置
# temperature=0, top_p=1, max_tokens=1000 → 准确、一致、完整
POST {{GLM_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: {{GLM_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "user",
      "content": "解释 Python 装饰器的原理和用法。"
    }
  ],
  "model": "GLM-4.7",
  "stream": false,
  "temperature": 0,
  "top_p": 1,
  "max_tokens": 1000
}

# ============================================
# 7. Stream vs 非 Stream 对比
# ============================================

### [流式 7.1] 非流式响应
# stream=false，等待完整响应后一次性返回
POST {{GLM_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: application/json
Authorization: {{GLM_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "user",
      "content": "写一个 200 字的故事。"
    }
  ],
  "model": "GLM-4.7",
  "stream": false,
  "temperature": 0.7
}

### [流式 7.2] 流式响应（SSE）
# stream=true，逐 token 返回，用户体验更好
POST {{GLM_BASE_URL}} HTTP/1.1
Content-Type: application/json
Accept: text/event-stream
Authorization: {{GLM_BEARER_TOKEN}}

{
  "messages": [
    {
      "role": "user",
      "content": "写一个 200 字的故事。"
    }
  ],
  "model": "GLM-4.7",
  "stream": true,
  "temperature": 0.7
}

# ============================================
# 总结
# ============================================
#
# 参数选择速查：
#
# Temperature（创造性）：0=确定 | 0.7=平衡 | 1.5=极创意
# Top-P（候选词）：0.1=保守 | 0.9=默认（通常不与 temperature 同调）
# Max Tokens：50=摘要 | 200-500=中等 | 1000+=详细
# Frequency Penalty：0=允许重复 | 1.0=避免重复
# Presence Penalty：0=话题集中 | 1.0=话题发散
#
# 场景推荐：
# 技术文档：temperature=0, max_tokens=1000
# 创意写作：temperature=0.8, frequency_penalty=0.5
# 头脑风暴：temperature=1.2, presence_penalty=1.5
# 客户服务：temperature=0.3, max_tokens=300
# ============================================
