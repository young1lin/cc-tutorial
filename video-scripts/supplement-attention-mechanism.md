# 补充：注意力机制 —— LLM 的核心引擎

> 本文从 `layer-01-theory.md` 第七章拆分而来，供想深入理解 Transformer 内部原理的读者参考。
> 对于只想用好 Claude Code 的读者，这部分不是必读内容——但如果你想知道"为什么长上下文贵""为什么不同模型编码能力差异大"，这里有答案。

---

## 7.1 Self-Attention（自注意力）与 O(n²) 的代价

最原始的注意力机制叫 **Full Self-Attention**：对于输入的每一个 Token，都要计算它和**所有其他 Token** 之间的关联度。

```
输入 Token 序列：[我, 今天, 写了, 一个, 函数, 它, 返回, 空值]

"它" 需要计算和前面每个 Token 的关联度：
  它 ↔ 我      → 0.02（弱关联）
  它 ↔ 今天    → 0.01（弱关联）
  它 ↔ 写了    → 0.05
  它 ↔ 一个    → 0.03
  它 ↔ 函数    → 0.82（强关联！"它"指的就是"函数"）
  它 ↔ 返回    → 0.15
  它 ↔ 空值    → 0.10
```

每个 Token 都要和其他所有 Token 两两计算，所以计算量是 **O(n²)**——Token 数量翻倍，计算量翻四倍。这就是为什么 200K 上下文窗口极其昂贵：200,000 × 200,000 = 400 亿次注意力计算。

## 7.2 多头注意力（Multi-Head Attention）

实际的 Transformer 不是只算一组注意力，而是**同时运行多组**，每组叫一个"头"（Head）。**[Author's analysis]** Claude 具体的注意力头数量未公开，但主流大模型通常有 64-128 个头。

**为什么要多个头？** 因为语言中的关联关系不止一种。单独一组注意力很难同时捕捉所有维度的关系，多个头各自分工，最后把结果拼起来。

**每个头学到了什么？** 研究发现（参见 Vaswani et al. 2017 "Attention Is All You Need"、Clark et al. 2019 "What Does BERT Look At?"），不同的头会**自发地**学会关注不同类型的语言关系：

| 头的类型 | 关注什么 | 示例 |
|---------|---------|------|
| **语法结构头** | 主谓宾、修饰关系 | "用户**提交**了表单" → 识别"用户"是主语、"表单"是宾语 |
| **指代消解头** | 代词指向谁 | "函数返回空值，**它**需要修复" → "它" = "函数" |
| **语义关联头** | 意思相近的词 | "bug"和"defect"关联度高 |
| **位置关系头** | 相邻 Token 的局部模式 | 关注前后 2-3 个词的搭配 |
| **长距离依赖头** | 跨越很远的关联 | 函数开头的参数定义 ↔ 函数末尾的 return 语句 |

**重要：这不是人为设计的分工，而是训练过程中自发涌现的。** 没有人告诉模型"第 7 个头负责语法"。模型在海量文本上训练时，不同的头自然演化出了不同的专长。这也意味着不是每个头都有清晰的功能——有些头的行为模式至今研究者也没有完全理解。

## 7.3 注意力机制的变体

Full Self-Attention 效果最好，但 O(n²) 太贵了。为了支持更长的上下文，各家模型做出了不同的取舍：

**1. 标准 Full Attention（因果）**
- 每个 Token 只看它前面的所有 Token（不偷看未来）
- 计算量 O(n²)，但效果最精确
- 短上下文（≤32K）的经典方案，代表：早期 GPT 系列

**2. 滑动窗口注意力（Sliding Window Attention, SWA）**
- 每个 Token 只看它前后固定范围内的 Token（比如前后 4096 个）
- 计算量降到 O(n·w)，w 是窗口大小
- 代价：超出窗口范围的信息会"看不到"

```
Full Attention（n=8）：每个 Token 看所有前面的
Token:  [1] [2] [3] [4] [5] [6] [7] [8]
  8看→   1   2   3   4   5   6   7   ✓（全部都看）

滑动窗口（w=3）：每个 Token 只看前面 3 个
Token:  [1] [2] [3] [4] [5] [6] [7] [8]
  8看→   ×   ×   ×   ×   5   6   7   ✓（只看最近 3 个）
```

**3. 滑动窗口 + Full Attention 混合**
- 大部分层用滑动窗口（省计算），每隔几层插一层 Full Attention（补回全局信息）
- **[Author's analysis]** 未经官方确认的架构信息不应作为事实陈述。StepFun 官方未公开 step-3.5-flash 的具体注意力机制细节。
- 效果：靠 Full Attention 层"中继"全局信息，让滑动窗口也能"间接看到"远处的内容

**4. 线性注意力（Lightning Attention）**
- 将 O(n²) 近似降到 O(n)
- 代价：注意力权重被"模糊化"，精确记忆能力下降
- **[Author's analysis]** 早期传言 MiniMax M2.1 使用线性+Full Attention 混合架构，但 MiniMax 官方博客明确说明 **M2 最终是 Full Attention 模型**。详见 [Why Did M2 End Up as a Full Attention Model?](https://www.minimax.io/news/why-did-m2-end-up-as-a-full-attention-model)（MiniMax CEO，2026）

**5. 双向非因果注意力**
- 每个 Token 可以同时看过去和未来
- 代表：**GLM-4.7**

**6. 工程优化技术（不改变注意力模式，但让长上下文可行）**
- **Flash Attention**（Tri Dao, 2022）：通过 IO 感知的分块计算，将显存从 O(n²) 降到 O(n)，但计算量仍是 O(n²)。几乎所有现代模型都在用。
- **Ring Attention**：将长序列切片分布到多张 GPU 上，每张卡只处理一段，卡间环形传递 KV。使百万级上下文在工程上可行。
- **GQA（Grouped Query Attention）/ MQA（Multi-Query Attention）**：多个注意力头共享同一组 Key 和 Value，大幅减少 KV Cache 的显存占用。Llama 2/3、Mistral 等公开采用。

## 7.4 Claude 的注意力机制：未知

**[Author's analysis]** Anthropic 至今没有发布 Claude 系列（包括 Claude Opus 4.6）的架构论文或技术报告，具体的注意力机制属于未公开信息。

根据 [Anthropic 官方公告](https://www.anthropic.com/news/claude-opus-4-6)（2026-02-05），Claude Opus 4.6 通过 API 和 pay-as-you-go 方式支持 **1M Token 上下文窗口**（标准订阅为 200K）。1M token 的完整注意力矩阵是 10^12 个元素，单层单头 FP16 就需要约 2TB 显存，这在工程上不可行——因此它几乎一定使用了上述优化技术的某种组合。

它几乎一定使用了上述优化技术的某种组合（Flash Attention + Ring Attention + GQA/MQA，以及可能的稀疏/混合注意力模式），但具体方案外界无从得知。GPT-4o 同理——OpenAI 也没有公开其架构细节。

**对我们的实际意义：** 不需要知道 Claude 的具体架构，只需要知道它在编码任务中的**实际表现**——长距离代码依赖的准确率、上下文利用率、在接近窗口上限时的质量衰减程度。这些可以通过实测评估，比架构猜测更有价值。

## 7.5 对编码场景的影响

为什么这些差异对 AI 编程很重要？因为写代码需要**精确的长距离记忆**：

- 文件开头 import 了什么 → 文件末尾能不能用
- 第 50 行定义的变量类型 → 第 300 行使用时类型要匹配
- 函数 A 的返回值 → 函数 B 调用时的参数校验

**已公开架构的模型**中，Full Attention 能精确记住这些关系，线性注意力会"模糊"掉，滑动窗口可能直接"看不到"。未公开架构的模型（Claude、GPT 等），则需要通过实际编码测试来评估其长距离记忆能力——从实测结果看，Claude Opus 4.6 在这方面表现优秀，但我们无法将其归因于某种具体的注意力机制。
